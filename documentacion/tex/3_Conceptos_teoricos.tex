\capitulo{3}{Conceptos teóricos}

\section{Minería de datos}
La minería de datos es el proceso de extraer información de grandes conjuntos de datos usando para ello técnicas de diversos campos, como la estadística, el aprendizaje automático o los sistemas de bases de datos. Aunque no es nueva, su uso durante los últimos años ha crecido de forma drástica. Varios factores explican este incremento. Por un lado, tenemos factores que han permitido generar enorme cantidad de datos, como la fuerte penetración de Internet en casi todo el mundo o la reducción del precio de los chips, permitiendo su incorporación a practicamente cualquier objeto y convirtiendolos, de facto, en ubícuos. Por el otro lado, tenemos la reducción del precio de almacenamiento. En 1956, el precio de un GB de disco duro tradicional costaba más de nueve millones de dólares. A día de hoy, ese precio se ha reducidos a menos dos centavos \cite{Evolucion-precio-gb}. 

Ahora bien, convertir toda esa cantidad de datos en información de la que se pueda extraer conocimiento no es tarea fácil, existiendo distintos métodos y metodologías para obtener resultados. Uno de estos métodos es CRISP-DM \cite{CRISP-DM}, \textit{Cross Industry Standard for Data Mining}, codificado en 1996 por un grupo de interés especial constituido tanto por empresas proveedoras de herramientas (SPSS y Teradata) cómo por empresas usuarias (Daimler, NCR y OHRA).

Este método se divide en seis pasos:
\begin{itemize}
	\item Entender el negocio, esto es, entender los objetivos y los requisitos del proyecto.
	\item Entender los datos, que incluye la recogida de los datos, así como su exploración inicial para detectar problemas de calidad o subconjuntos para formular hipótesis.
	\item Preparar los datos para solventar cualquier problema detectado en la fase previa y facilitar su procesado por los modelos.
	\item Modelar, donde se definen las técnicas y arquitecturas que se van a usar y se construyen los modelos. Desde este punto es posible volver a la fase previa de preparación de los datos, ya que diferentes modelos pueden necesitar diferentes técnicas de procesado de los datos.
	\item Evaluar los modelos generados y seleccionar finalmente uno o varios de ellos, en función de las necesidades.
	\item Desplegar el modelo seleccionado de forma que pueda ser usado para realizar predicciones sobre nuevos datos.
\end{itemize}

Veamos a continuación como se aplican las diferentes etapas del proceso CRISP-DM en nuestro proyecto.

\imagen{CRISPDM_Process_Diagram.png}{Fases del método CRISP-DM \cite{CRISP-DM}}

\subsection{Entendiendo el negocio: objetivos y requisitos}

Ya hemos comentado en el capítulo previo los \nameref{sec:objetivos-del-proyecto}, por lo que no los repitiremos aquí.

\subsection{Entendiendo los datos}

Los datos para este proyecto han sido descargados de \href{https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data}{Kaggle}, donde podemos encontrar su descripcion: "los datos describen el cambio en el flujo (intensidad de luz) de varios miles de estrellas. Cada estrella tiene una etiqueta binaria de 2 o 1. Un 2 indica que se confirmó que la estrella tiene al menos un exoplaneta en órbita; algunas observaciones son, de hecho, sistemas de varios planetas." \cite{Kaggle-exoplanet}

La fuente original de los datos pertenecen a la misión Kepler de la NASA, concretamente a la campaña 3 de la fase K2. Se pueden encontrar en la web del Mikulski Archive \href{https://archive.stsci.edu/k2/}{Mikulski Archive} \cite{Mikulski-Archive}. Los datos originales han sido transformados por \href{http://www.danlessmann.com/index.htm}{Dan Lessmann}; el proceso se encuentra detallado en su \href{https://github.com/winterdelta/KeplerAI}{repositorio en GitHub}. Finalmente, los datos han sido incorporados a Kaggle, desde donde han sido descargados.

Nuestro dataset se compone de dos archivos, uno con los datos de entrenamiento y otro con los datos para testear. Su composición es la siguiente:

- Fichero de entrenamiento:
\begin{itemize}
	\item 5087 filas u observaciones
	\item 3198 columnas o características
	\item La primera columna es la etiqueta para clasificación. Las columnas 2-3198 son los valores de flujo a lo largo del tiempo
	\item Hay 37 estrellas con exoplanetas confirmados y 5050 estrellas sin exoplanetas
\end{itemize}

- Fichero de test:
\begin{itemize}
	\item 570 filas u observaciones
	\item 3198 columnas o características
	\item La primera columna es la etiqueta para clasificación. Las columnas 2-3198 son los valores de flujo a lo largo del tiempo
	\item Hay 5 estrellas con exoplanetas confirmados y 565 estrellas sin exoplanetas
\end{itemize}

Lo primero que se puede observar es que un dataset enormemente desbalanceado. Las estrellas con exoplanetas confirmados solo representan el 0.73\% en el dataset de entrenamiento y el 0.88\% en el de test, lo que representa un grave handicap para el entrenamiento exitoso de los modelos. Veremos posteriormente técnicas para lidiar con este problema.

Vamos a proceder a mostrar gráficamente algunos ejemplos de estrellas con y sin exoplanetas para intentar captar características comunes o posibles anomalías.

\imagen{estrellas_con_exoplanetas.png}{Estrellas con exoplanetas confirmados}

\imagen{estrellas_sin_exoplanetas.png}{Estrellas sin exoplanetas confirmados}

Observando las imágenes destacan dos cosas. Por un lado, la amplitud en el rango de la intensidad de luz y, por otro, la presencia puntual de picos de luz. Esta es una circunstancia extraña, ya que el brillo de una estrella suele permanecer relativamente estable. Y, desde luego, no es debido al transito de una planeta, ya que esto reduciria el valor, como se aprecia en los picos descendentes de las primeras graficas. Asi pues, lo mas probable es que estos valores sean errores y lo mejor será descartarlos durante el procesado de los datos.

\subsection{Preparación de los datos}

Rara vez los datos se recogen de forma que puedan ser evaluados de forma directa por un modelo, por lo que necesitan de un tratamiento previo que haga más factible extraer información de ellos. Por supuesto, no todos los datos van a ser tratados de idéntica forma y, dependiendo del estado y del objetivo del proyecto, habrá que utilizar unas técnicas u otras, aunque hay algunas cuyo uso está tan extendido que su uso se ha convertido en un paso obligatorio de casí cualquier proyecto.

Una de estás técnicas es la normalización, cuyo objetivo es cambiar los valores de las columnas numéricas de forma que usen una escala común. Esto nos permitirá abordar uno de los problemas detectados anteriormente, la amplitud en el rango de valores de nuestros datos.

Además de la normalización, veremos más adelante otra serie de técnicas que nos ayudaran a entrenar el modelo, tales como \textit{undersampling} y \textit{oversampling}, uso de filtros para suavizar la señal o el estudio de la frecuencia.

\subsection{Modelado}

En esta fase del proceso es cuando se definen los modelos y arquitecturas que se usaran. Nosotros nos centraremos especialmente en el uso del \nameref{sec:perceptron-multicapa} y, en menor medida, las redes \nameref{sec:lstm}, un tipo de redes neuronales recurrente. 

\subsection{Evaluación}

Una vez tenemos los modelos entrenados, es el momento de evaluarlos y seleccionar el mejor. La diferencia de objetivos y de características de los datos, puede llevar a emplear métricas muy diversas para evaluarlos. En el caso que nos atañe, donde pretendemos construir un clasificador binario, nos centraremos especialmente en evaluar la sensibilidad y especificidad de nuestro modelo. Entraremos en más detalle en \nameref{sec:metricas}.

\subsection{Despliegue}

El paso final del proceso incluy la puesta en producción del modelo elegido de forma que pueda usarse para obtener predicciones reales sobre nuevos datos. Dependiendo del proyecto, algunas opciones habituales incluyen despliegues en aplicaciones webs, aplicaciones móviles o su contenerización.

\section{Normalización}
Como hemos comentado anteriormente, normalizar consiste en transformar los valores numéricos de una o más columnas para que sus valores se ajusten a una escala común y es más importante cuanto más amplio sea el rango de valores que adopta el dataset.  

Normalizar no es un proceso único, sino que existen diversas técnicas para normalizar, cada una con ventajas y desventajas asociadas por lo que, en función de los datos, puede ser conveniente usar una u otra. Las dos más usadas son la normalización mínimo-máximo y la puntuación z. La primera de ellas tiene la ventaja de transformar todos los valores al rango [0, 1] siendo sensible a valores atípicos extremos mientras que la puntuación z, menos sensibles a estos valores extremos, no garantiza un rango fijo.

\begin{math}
	minmax = \frac{X - min(x)}{max(x) - min(x)}
\end{math}

\begin{math}	
	zscore = \frac{X - mean(x)}{stdev(x)}
\end{math}

\section{Filtro gaussiano}\label{filtro-gaussiano}
Dada la elevada sensibilidad del satélite Kepler a la intensidad de luz y, como se observa en las imagenes previas, las señales presentan numerosos picos y valles que pueden dificultar el aprendizaje de los modelos. Para solucionar este problema, usaremos un filtro gaussiano, un efecto de suavizado que puede aplicarse a una señal o imagen para reducir el ruido.

A la hora de aplicarlo tenemos dos opciones distintas. La primera es aplicar el filtro tal cúal, provocando el suavizado la señal, quitandole ruido, pero también nitidez y detalles; detalles que pueden ser importantes. La otra alternativa consiste reducir la señal original con el resultado de aplicar el filtro gaussiano, consiguiendo de esta forma reducir la importancia de la señal base y resaltando los detalles más especificos.

\section{Undersampling y Oversampling}\label{undersampling-y-oversampling}
Cuando, como es nuestro caso, tenemos un dataset fuertemente desbalanceado, es necesario aplicar técnicas que permitan el correcto entrenamiento de la red. Dos de ellas son \textit{undersampling} y \textit{oversampling}.

La primera de ellas consiste en reducir el tamaño de nuestro dataset, normalmente hasta que tengamos el mismo número de ejemplos de cada clase, aunque pueden usarse otras proporciones. La técnica habitual consiste en seleccionar de forma aleatoria elementos de clase mayoritaria y eliminarlos. Una ventaja derivada de este método es que, al tener que procesar un menor número de elemtnos, el entrenamiento es más rapido. Entre las desventajas tenemos, en primer lugar, que al privar a la red de determinados ejemplos le estamos ocultando información que puede ser importante y provocar que la red no generalize bien. De forma similar, cuando el número de ejemplos de una clase es muy pequeño, como es nuestro caso, el dataset puede ser demasiado pequeño, causando que la red memorice los datos y sea incapaz de generalizar.

La otra técnica, \textit{oversampling}, consiste en el proceso opuesto, esto es, generar nuevos elementos de clase minoritaria. Igualmente, se suelen generar ejemplos hasta conseguir la igualdad, aunque no siempre es el caso. A la hora de generar los nuevos ejemplos, el enfoque más sencillo y rápido consiste en realizar copias de los datos de la clase minoritaria. La desventaja evidente de este proceso es que la red no incorpora nueva información y, al procesar los mismos datos una y otra vez, es más probable que acabe memorizandolos y, por tanto, sin capacidad de generalizar ante datos nuevos.

Para tratar de solucionar este problema existen diversos métodos, siendo uno de los más utilizados SMOTE \cite{SMOTE} (\textit{synthetic minority oversampling technique}). Aunque existen multiples variantes, la forma básica para generar nuevos elementos consiste en, para cada elemento de la clase minoritaria, se seleccionan sus K (generalmente, 5) vecinos más próximos. De entre ellos, se selecciona uno al azar y se generan una más instancias (dependiendo del número total de instancias a generar) en la recta que une las instancias originales.

\section{Transformada de Fourier}
La transformada de Fourier es una transformación matemática que permite descomponer una función, generalmente expresada en función del tiempo, en sus frecuencias constitutivas. En nuestro caso, el estudio de la frecuencia puede ser interesante dado que la existencia de uno o más planetas orbitando la estrella resultaría en la existencia de más de una frecuencias.

\imagen{frecuencias.png}{Ejemplo de una señal (arriba, en amarillo) y su descomposición en las frecuencias que la componen \cite{3blue1brown}}

El nombre de transformada de Fourier, en honor al matemático francés Joseph Fourier, hace referencia tanto a la representación como a la función que la produce. Es, además, una operación reversible, permitiendo pasar de un dominio a otro. 

El análisis matemático de la transformada de Fourier está fuera del alcance de este trabajo, pero es posible encontrarlo, junto con información más detallada en diferentes fuentes como, por ejemplo, la \href{https://es.wikipedia.org/wiki/Transformada_de_Fourier}{wikipedia} \cite{Wikipedia-fourier}.

\section{Perceptrón Multicapa}\label{sec:perceptron-multicapa}

El perceptrón multicapa, o red multicapa con propagación hacia delante,
es el modelo de aprendizaje profundo por excelencia. Es una
generalización del perceptrón simple que surgió debido a la incapacidad
de estos para dar solución a problemas no lineales \cite{Minsky-1969}.

El objetivo de estas redes es aproximar alguna función \emph{f}. Por
ejemplo, para un clasificador como es nuestro caso,
\texttt{y = f(x)} mapea un input, \emph{x} a una categoría \emph{y}.
La red define un mapeado \texttt{y = f(x, $\theta$)} y aprende los valores de los parametros $\theta$ que resultan en la mejor aproximación a la función \cite{Goodfellow-et-al-2016}.

Su arquitectura es simple, consistiendo en una capa de entrada,
encargada de recibir las señales del exterior y propagarlas a las
neuronas de la siguiente capa, una o más capas ocultas, que procesan la
información, aplicando una función de activación a los datos recibidos
de la capa previa, y una capa de salida, que comunica al exterior la
respuesta de la red.

\imagen{perceptron-multicapa.png}{Perceptrón multicapa}

\subsection{Consideraciones de diseño}\label{consideraciones-de-diseno}

A la hora de diseñar la arquitectura de un perceptrón multicapa hay
varios elementos que podemos alterar para tratar de lograr una mejor
solución.

\subsection{Número de neuronas}\label{numero-de-neuronas}

En algunos casos, especialmente en la capa de entrada y de salida, el
número de neuronas viene definido por el problema a resolver. En las
capas ocultas, este número puede variar ampliamente. Hay que considerar
que un número elevado de neuronas puede provocar que estas memorizen los
datos de entrada, proceso conocido como \emph{overfitting}. En este
caso, nuestra red proporcionaría buenos resulados durante con los datos
entrenamiento, pero sería incapaz de generalizar y los resultados serian
pobres cuando se enfrentase a datos nuevos.

Por otro lado, un número escasos de neuronas puede provocar el efecto
contrario, que nuestra red no disponga de la capacidad necesaria para
generalizar correctamente. En este caso, conocido como
\emph{underfitting}, la red presenta pobres resultados, tanto en el
entrenamiento como en los tests.

En nuestro caso, el número de neuronas en la capa de entrada viene
determinado, a priori, por el número de características de nuestro
dataset, esto es, 3197. Respecto a la capa de salida, dependerá de la
función de activación que se vaya usar; en el problema que tratamos de
resolver, clasificando los datos en dos categorías, hay o no hay
exoplaneta, usaremos dos neuronas.

\subsection{Número de capas y conexiones}\label{numero-de-capas-y-conexiones}

De forma similar al número de neuronas, el número de capas ocultas puede
variar ampliamente, contribuyendo especialmente al problema de
\emph{overfitting} comentado anteriormente. Además, de acuerdo al
teorema de aproximación universal, cuando se usan funciones de
activación no lineales, una sola capa oculta es suficiente para
representar cualquier función continua en un rango dado, aunque esta
capa puede ser demasiado grande y fallar en aprender y generalizar
correctamente \cite{Goodfellow-et-al-2016}, por lo que es conveniente probar varias
aproximaciones. Dado que nuestro problema es, además, no continuo, no
debemos ceñirnos a usar una sola capa oculta.

Es también importante como se encuentran conectadas las capas. El modelo
es más frecuente es el de capa totalmente conectada, donde cada neurona
esta conectada a cada una de las neuronas de la siguiente capa. Hay, sin
embargo, otras opciones donde, la más frecuente de ellas, consiste en
que la salida de algunas o todas las neuronas de una capa se conectan
con la entrada de neuronas de una capa no inmediatamente posterior,
haciendo que su valor tenga más peso en el resultado final de la red.

Otro opción respecto a las conexiones entre las capas es usar la técnica
conocida como \emph{dropout}. Esta consiste en asignar, durante el
proceso de entrenamiento, el peso de determinadas neuronas,
seleccionadas de forma aleatoria, a cero, excluyendo de facto su
aportación al resultado final de la red. El objetivo de la técnica es
reducir el \emph{overfitting}, ya que hace que la red sea menos
dependiente del peso especifico de determinadas neuronas \cite{JMLR:v15:srivastava14a}.

\subsection{Funciones de activación}\label{funciones-de-activacion}

Vamos a examinar brevemente las funciones de activación más frecuentes
que podríamos usar en nuestro modelo.

La función sigmoide fué de las primeras en usarse de forma masiva. La
función está acotada entre {[}0, 1{]} y suele usarse en la última capa
para representar probabilidades en clasificadores binarios. También es
habitual usarla en las capas ocultas de los perceptrones multicapa. Sin
embargo, adolece de algunos problemas, quizá el mayor de ellos sea que
satura y mata el gradiente, provocando una lenta convergencia.

La tangente hiperbólica es muy similar a la sigmoide, estando igualmente
acotada, aunque en un rango mayor, {[}-1, 1{]}, lo que la hace adecuada
para problemas en los que hay que decidir entre dos opciones. A
diferencia de la sigmoide, esta centrada en el 0.

Es importante resaltar que la función sigmoide y la tangente hiperbólica
se encuentran relacionadas, tal que \(tanh(x) = 2 * sigmoid(2x) -1\) por
loq que existe poca diferencia a la hora de usar una u otra.

Tenemos también la función relu, función lineal rectificada por sus
siglas en inglés. Esta función no está acotada y deja los valores
positivos sin alterar pero transforma los negativos en cero. Tiene un
buen desempeño en redes convolucionales a la hora de tratar con
imagenes, pero también es la opción por defecto en los perceptrones
multicapa. Esto se debe principalmente a dos factores: es poco probable
que mate el gradiente y genera redes escasas, esto es, redes con
neuronas muertas que no se activan, lo que hace la red más eficiente.
Además, su facilidad de cálculo respecto a otras funciones, acorta el
tiempo de entrenamiento de la re. Presenta, sin embargo, un importante
problema, y es que puede matar a demasiadas neuronas. Para solucionarlo,
existe una variante, denominada leaky relu, que, en lugar de anular los
valores negativos, los multiplica por un coeficiente para devolver un
valor negativo.

Finalmente, hablamos de la funcion softmax, que transforma un vector de
entrada en un vector de probabilidades cuyo sumatorio es uno. Es usada
frecuentemente en la capa de salida de la red cuando se trata de
resolver un problema de clasificación.

De cara al diseño de nuestro modelo, la elección evidente para la capa
de salida es usar una función softmax, aunque también se realizarán
pruebas usando la función sigmoide para ver si presenta un mejor
resultado.

Respecto a las capas ocultas, usaremos principalmente la función relu y,
de forma similar a con la capa de salida, haremos pruebas con la
sigmoide.

\subsection{Algoritmo de optimización}\label{algoritmo-de-optimizacion}

El algoritmo de optimización es el encargado de actualizar los pesos de
nuestra red para minimizar la perdida. Pytorch ya tiene implementados
varios de estos algoritmos, por lo que solo queda decidir cual usar.

El más conocido y uno de los primeros en desarrollarse es el descenso
del gradiente. Pytorch implementa el descenso del gradiente estocástico
(SGD), que actualiza los pesos tras procesar cada ejemplo, en lugar de
hacerlo tras procesar todo el dataset. Este algoritmo presenta algunas
dificultades, como elegir la tasa de aprendizaje adecuada y que ese
valor se aplique a todos los pesos por igual, así como oscilaciones que
dificultan la convergencia en el punto mínimo. Para intentar corregir
este último problema, el algoritmo puede configurarse para usar momento,
que ayuda a suavizar las oscilaciones añadiendo una fracción de los
pasos previos al paso actual.

Usaremos este optimizador como linea base de trabajo con diferentes
valores para la tasa de aprendizaje tanto con como sin momento.

Otro de los algoritmos más usados es Adam \cite{2014arXiv1412.6980K}, acrónimo en inglés de
estimación adaptativa del momento, que será el otro algoritmo que
usaremos.

A diferencia de SGD, Adam calcula tasas de aprendizaje distintas para
los parámetros e incorpora momento. Adam es computacionalmente
eficiente, tiene pocos requisitos de memoria y facilita la convergencia.
Además, al actualizar los parámetros con diferentes tasas de
aprendizaje, es menos sensible a una elección no óptima de la tasa de
aprendizaje inicial.

\section{LSTM}\label{sec:lstm}

Un tipo particular de redes neuronales son las recurrentes, que permiten añadir la dimensión temporal al procesado de datos. Esto lo consiguen incluyendo bucles en su arquitectura, permitiendo de esta forma la persistencia de determinada información y dotandolas de una gran facilidad para tratar información contenida en forma de secuencias o listas, donde un acontecimiento esta estrechamente vinculado a acontecimientos previos. Esto las ha convertido en el estandar para tratar diversos tipos de problemas como, por ejemplo, el reconocimiento de voz, el modelado del lenguaje o la traducción entre idiomas.

Para nuestro problema particular, este tipo de red puede resultar útil, ya que el transito planetario incluye una sencuencia concreta de pasos, donde el flujo de luz se muestra relativamente constante, posteriormente experimenta un decrecimiento hasta llegar un mínimo y finalmente vuelve a su nivel original. Además, si el periodo orbital del planeta es breve, podríamos encontrar repeticiones de esta secuencia. El procesado de este conjunto de datos temporales es la especialidad de las redes recurrentes.

\imagen{transit.png}{Variación de la intensidad de luz por tránsito planetario \cite{Kaggle-exoplanet}}

La forma más simple de conseguir esta recurrencia es conectando la salida de una neurona no solo hacia neuronas de las capas siguientes, como hemos visto en el caso del perceptrón multicapa, sino también hacia la entrada de la propia neurona o de neuronas de las capas previas. Esto es, la salida de las neuronas de la capa $h_t$ se encuentran conectadas a las neuronas de la capa $h_{t-i}$. 

Esta configuración simple funciona muy bien para tratar secuencias donde la información se encuentra temporalmente próxima pero presenta problemas relacionando información distante en el tiempo o información que influye en muchos momentos pasados.

Pero existen muchos tipos de redes recursivas. En nuestro caso nos centraremos en las redes LSTM (\textit{long short term memory}), uno de los estandares de la industria, y designadas con el objetivo de presentar memoria a largo plazo. La arquitectura básica de estas redes esta formada por capas LSTM, también llamadas células, cada una sirviendo de entrada a la célula siguiente. En su interior, cada una de estas células presenta los mismos elementos, siendo el principal $C_t$, el estado de la célula y la parte que proporciona la memoria a largo a plazo al sistema.

Otros elementos influyen sobre esta memoria. Una primera capa sigmoidal permite determinar que memoria a largo plazo mantener o desechar. Otras capas interiores, implementadas mediante funciones sigmoidales y tangenciales, permite determinar que información actual incorporar a la memoria a largo plazo. Finalmente, las capas finales se encargan de devolver la información procesada por la célula y de conectar su estado a la siguiente célula de la red.

Para una descripción más detallada del proceso, se puede consultar \href{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}{Understanding LSTMs}.

\imagen{LSTM.png}{Detalle de una célula LSTM \cite{LSTMs}}

\section{Medidas de desempeño del modelo}\label{sec:metricas}

El objetivo de nuestro modelo es obtener un clasificador binario que nos
indique la probabilidad de que una entrada de datos pertenezca a una de
nuestras dos clases: exoplaneta y no exoplaneta.

Con este objetivo en mente, la literatura existente nos indica que la
solución óptima suele ser aplicar una función de activación softmax para
la capa de salida de la red. Esta función, también llamada función
exponencial normalizada, es una forma de regresión logística que
normaliza un valor de entrada en un vector de salida que sigue una
distribución de probabilidad cuya suma total es 1. Así pues, el valor de
salida de la neurona k-ésima vendrá dado por la función:

\begin{math}
s(x_{i})=\frac{e^{x_{i}}}{\sum_{j=1}^{n}e^{x_j}}
\end{math}

En cualquier caso, estudiaremos otras opciones, como puede ser el caso
de la función sigmoide, que nos devuelve un valor en el rango {[}0,
1{]}, el cual puede ser interpretrado como probabilidad, en nuestro
caso, de que sea una estrella con exoplaneta.

\subsection{Desbalanceo del dataset}\label{desbalanceo-del-dataset}

Analizando nuestro conjunto de datos, observamos que este se encuentra
muy desbalanceado: los casos negativos (no es un exoplaneta) superan
ampliamente en número a los casos positivos (si es un exoplaneta).

Esto supone un problema para el aprendizaje de la red ya que, ante
cualquier entrada, esta puede \emph{``aprender''} a responder siempre
que no es un exoplaneta, acertando en la amplia mayoría de los casos.

Para solventar este problema, siguiendo a Viloria \cite{Viloria-2006}, vamos a
definir nuestra función de evaluación, con la que juzgaremos el
aprendizaje real de nuestra red y su capacidad de predecir el resultado
correcto frente a nuevas entradas de datos.

\begin{math}
f = Acierto * (\alpha * Sen + \beta * Esp)
\end{math}

donde \texttt{Acierto} representa el ratio de respuestas correctas,
\texttt{Sen} es la sensibilidad (casos catalagocados como positivos
que son realmente positivos), \texttt{Esp} es la especificidad
(casos negativos correctamente calificados como no exoplanetas) y
\(\alpha\) y \(\beta\) son dos pesos usados para alterar la importancia de la sensibilidad y la
especificidad. Comenzaremos con un valor neutro de 0.5 para cada uno,
pero estudiaremos si su ajuste permite obtener un mejor modelo.

Definimos a continuación los ratios de \texttt{Acierto},
\texttt{Sen} y \texttt{Esp}, donde \texttt{VP} es el número
de verdaderos positivos, \texttt{VN} los verdaderos negativos,
\texttt{FP} los falsos positivos y \texttt{FN} los falsos
negativos.

\begin{math}
Acierto = \frac{VP + VN}{VP + VN + FP + FN}
\end{math}

\begin{math}
Sen = \frac{VP}{VP + FN}
\end{math}

\begin{math}
Esp = \frac{VN}{VN + FP}
\end{math}

\section{Bibliotecas de machine learning}\label{sec:bibliotecas-de-machine-learning}

De cara a implementar nuestro modelo de red neuronal debemos decidir que
lenguajes y bibliotecas vamos a usar. A día de hoy, la mayor parte de los
frameworks y bibliotecas que facilitan el desarrollo de redes neuronales
funcionan en entornos Python, que puede ser considerado el lenguaje de
facto de la industria, aunque existen otras alternativas en lenguajes
como R, Mathlab, y en frameworks como Neuroph para Java o Mathematica.

En nuestro caso, vamos a considerar una comparativa de tres bibliotecas de Python:

\begin{itemize}
	\itemsep1pt\parskip0pt\parsep0pt
	\item
	Tensorflow es una biblioteca de código abierta desarrollada por Google
	para uso interno, tanto en investigación como en producción, que
	posteriormente fue lanzada al público.\\
	\item
	Keras es una API de alto nivel capaz de ejecutarse sobre otros
	lenguajes o bibliotecas, como Tensorflow, R o Theano, diseñada con el
	foco en la facilidad de uso.\\
	\item
	PyTorch es una biblioteca de código abierta desarrollada principalmente
	por Facebook que también presenta una interfaz para C++.
\end{itemize}

Vamos a examinar diferentes parámetros para ver que nos aporta cada una
de ellas.

\subsection{Velocidad}\label{velocidad}

Los estudios muestran que no hay una diferencia significativa de
velocidad entre Tensorflow y PyTorch. Este no es el caso con Keras, que
presenta un rendimiento claramente inferior.

\subsection{Nivel del API}\label{nivel-del-api}

Como se ha comentado, Keras es una API de alto nivel, capaz de correr
sobre otras bibliotecas, como Tensorflow o Theano, proporcionando una
interfaz común que facilita el desarrollo rápido de proyectos.

Tensorflow proporciona APIs tanto de alto como de bajo nivel, lo que le
dota una gran flexibilidad.

Finalmente, PyTorch proporciona sólamente una API de nivel, enfocada en
el trabajo directo con matrices.

\subsection{Arquitectura}\label{arquitectura}

Keras presenta una arquitectura simple y fácil de comprender, mientras
que tanto Tensorflow como PyTorch presentan arquitecturas más complejas
y un código con mayor verbosidad.

La API de PyTorch se encuentra mejor diseñada mientras que la de
Tensorflow es un tanto confusa y ha recibido numerosos cambios
importantes en cada versión, lo que dificulta mantener un código estable
y estar actualizado.

\subsection{Debuggin}\label{debuggin}

Depurar código en Tensorflow es relativamente complejo y no muy
intuitivo. En Keras no es un proceso habitual, dado el alto nivel de sus
componentes, lo que tampoco facilita la depuración en caso de algún
problema, ya que la mayor parte del código se encuentra en la biblioteca.
Sin embargo, PyTorch si ofrece buenas opciones para la depuración, muy
similares a las encontradas en IDEs para lenguajes conocidos, como
Eclipse o Visual Studio.

\subsection{Dataset}\label{dataset}

Los problemas de velocidad de Keras no lo hacen adecuado para trabajar
con grandes datasets. No es el caso de Tensorflow o PyTorch, que no
tienen este problema de rendimiento.

\subsection{Documentación y comunidad}\label{documentacion-y-comunidad}

Tanto en PyTorch como en Tensorflow se nota el efecto de tener detras a
dos grandes empresas tecnológicas. En ambos casos, existen númerosos
recursos gratuitos con los que aprender así como una importante
comunidad de usuarios que las respaldan y ofrecen su ayuda. Tensorflow
tiene más tiempo de desarrollo y su base de usuarios es mayor pero desde
el 2018 la popularidad de PyTorch está en constante aumento,
especialmente en el ambito académico.

Keras contrasta respecto a las otros dos con una más reducida comunidad
y menor documentación.

\subsection{Puesta en producción}\label{puesta-en-produccion}

A la hora de poner en producción un modelo previamente entrenado, Keras
no dispone de ninguna utilidad en si misma, haciendo uso de las
caracteristicas de Tensorflow. Este permite servir los modelos en un
servidor web mediante una API REST o en dispositivos móviles.

PyTorch se apoya en otras bibliotecas para poder exponer sus modelos via
web, permitiendo también otras opciones interesantes, como la interfaz
con C++, lo que permite convertir los modelos en ejecutables fácilmente.

\subsection{Resumen de la comparación }\label{decision-final}

Tras analizar las características de las tres bibliotecas, vemos como se
adaptan a nuestras necesidades.

Keras es una buena opción para probar y generar modelos de forma rápida,
pero no nos permite profundizar en el aprendizaje y compresión de las
redes neuronales, ya que la mayor parte del trabajo de nivel es
gestionado de forma interna por la biblioteca. Unido a la dificultad de
depurar el código y a su peor rendimiento, hace que optemos por no
utilizarla.

La decisión entre Tensorflow y PyTorch es más dificil de realizar, ya
que ambos aportan características similares: la posibilidad de trabajar
con las redes a bajo nivel para poder estudiarlas en detalle, buen
rendimiento y variados recursos para aprender, ya sea en forma de
tutoriales o de comunidad de usuarios para resolver dudas. Sin embargo,
hay dos detalles marcan la diferencia y nos hacen decantarnos por
PyTorch: por un lado, la facilidad de depuración del código y, por otro,
la posibilidad de generar ejecutables.

Así pues, la biblioteca que finalmente usaremos será \textbf{PyTorch}.
